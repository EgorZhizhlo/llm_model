```
   ____                    _____                     _     _    _                 _ _           
  / __ \                  / ____|                   | |   | |  | |               | | |          
 | |  | |_ __   ___ _ __ | (___   ___  __ _ _ __ ___| |__ | |__| | __ _ _ __   __| | | ___ _ __ 
 | |  | | '_ \ / _ \ '_ \ \___ \ / _ \/ _` | '__/ __| '_ \|  __  |/ _` | '_ \ / _` | |/ _ \ '__|
 | |__| | |_) |  __/ | | |____) |  __/ (_| | | | (__| | | | |  | | (_| | | | | (_| | |  __/ |   
  \____/| .__/ \___|_| |_|_____/ \___|\__,_|_|  \___|_| |_|_|  |_|\__,_|_| |_|\__,_|_|\___|_|   
        | |                                                                                     
        |_|                                                                                     
```

# ИИ модуль

Модуль представляет собой RESTful API-сервис для обработки файлов и 
предоставления ответов с использованием RAG (Retrieval Augmented Generation). 
Сервис развернут в Docker-контейнере и использует FastAPI для создания API, 
OpenSearch для индексирования данных и OLLAMA для генерации текста на основе запросов к индексу.

# Запуск
Для запуска необходимо развернуть docker контейнеры. 
Для этого необходимо прописать две команды: 
 - `docker compose build` - для сборки
 - `docker compose up -d` - для запуска контейнеров

После этого надо будет запустить LLM в контейнере `llama`. 
Выполните следующую команду чтобы войти внутрь контейнера llama. 

`docker exec -it <имя_контейнера_llama> bash`

Замените `<имя_контейнера_llama>` на реальное имя вашего контейнера, которое можно узнать с помощью команды `docker ps`.
Внутри контейнера необходимо выполнить команду `ollama run llama3`, 
после которой установится и запустится большая языковая модель. Остаётся только прописать `/bye` и выйти из контейнера.

# Использование
Модуль не предназначен для использования пользователем

# Описание работы

## class OpenSearchHandler
Класс `OpenSearchHandler` предназначен для работы с моделью языка (LLM) через векторный поиск на основе OpenSearch. 
Он позволяет загружать документы, разбивать их на части, 
индексировать эти части вместе с векторными представлениями и затем использовать их при генерации ответов от LLM. 

### Методы класса:

#### `__init__`
Конструктор класса, инициализирующий клиент OpenSearch, модель встраивания и модель языка.

#### `_initialize_index`
Создает новый индекс в OpenSearch для конкретной сессии, если он еще не существует. Этот метод также настраивает карту индексов для поддержки KNN-поиска.

#### `add_documents`
Этот метод принимает текстовый документ, разбивает его на части с помощью `CharacterTextSplitter`, создает векторные представления этих частей и добавляет их в индекс OpenSearch.

#### `invoke_llm`
Основной метод для взаимодействия с моделью языка. Он проверяет наличие индекса для текущей сессии, выполняет векторный поиск релевантных фрагментов текста, формирует контекст для модели языка и возвращает ответ.

#### `_format_docs`
Вспомогательный метод для форматирования списка документов в одну строку, чтобы передать ее в качестве контекста для модели языка.

# main.py
Веб-приложение на базе фреймворка FastAPI, которое предоставляет API для добавления документов в индекс OpenSearch и 
получения ответов от модели языка (LLM) на основе этих документов. 

### Модельные классы:
- `AddDocuments`: Схема для приема данных о сессии и тексте документа.
- `LlmInvokes`: Схема для приема данных о сессии, вопросе и базовом шаблоне промта.

### Маршруты API:

#### `/add-document`
Метод POST для добавления нового документа в индекс OpenSearch. Принимает JSON с параметрами `session_token` и `text`. 
Если операция прошла успешно, возвращается статус-код 200. В случае ошибки выбрасывается исключение `HTTPException` с подробным сообщением об ошибке.

#### `/invoke_llm`
Метод POST для вызова модели языка с использованием векторного поиска в OpenSearch. 
Принимает JSON с параметрами `session_token`, `question` и опциональный параметр `base_prompt`. Возвращает ответ от модели языка в формате JSON. 
В случае ошибки выбрасывается исключение `HTTPException` с подробным сообщением об ошибке.

### Общая структура:
Приложение настроено таким образом, что модуль backend может добавлять документы в индекс OpenSearch, 
ассоциируя их с конкретными сессиями (`session_token`), а затем запрашивать ответы у модели языка на основе этих документов. 
Это позволяет создавать персонализированные чаты или системы помощи, где информация, предоставленная пользователем ранее, учитывается при формировании ответа.
